{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy  as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, GridSearchCV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Importing our train & test dataset\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe().round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe(include=['O']) # Describes categorical variables","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#EDA\n# Mean survival by passenger class\ntrain_df.groupby(['Pclass'], as_index=False)['Survived'].mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mean survival by sex\ntrain_df.groupby(['Sex'], as_index=False)['Survived'].mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mean survival by number of siblings / spouses aboard the Titanic\t\ntrain_df.groupby(['SibSp'], as_index=False)['Survived'].mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mean survival by number of parents / children aboard the Titanic\ntrain_df.groupby(['Parch'], as_index=False)['Survived'].mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# By family size = SibSp + Parch + 1\ntrain_df['Family_Size'] = train_df['SibSp'] + train_df['Parch'] + 1\n#test_df['Family_Size'] = train_df['SibSp'] + train_df['Parch'] + 1\ntest_df['Family_Size'] = test_df['SibSp'] + test_df['Parch'] + 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mean survival by family size\ntrain_df.groupby(['Family_Size'], as_index=False)['Survived'].mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grouping by Family_Size\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6:'Medium', 7:'Large', 8:'Large', 9:'Large', 10:'Large', 11:'Large'}\ntrain_df['Family_Size_Grp']= train_df['Family_Size'].map(family_map)\ntest_df['Family_Size_Grp']= test_df['Family_Size'].map(family_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mean survival by family Groups\ntrain_df.groupby(['Family_Size_Grp'], as_index=False)['Survived'].mean().round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mean survival by Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\ntrain_df.groupby(['Embarked'], as_index=False)['Survived'].mean().round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Building Graphs\n# Disburition of age by survival\nsns.displot(train_df, x='Age', col='Survived', binwidth=10, height=5) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.displot(train_df, x='Age', height=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By age groups of 8 bins\ntrain_df['Age_Cut'] = pd.qcut(train_df['Age'], 8) #splits a age variable into quantile-based bins(intervals that each contain the apprx same number of data points)\ntest_df['Age_Cut'] = pd.qcut(test_df['Age'], 8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['Age_Cut'], as_index=False)['Survived'].mean().round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reclassifying age from 0 to 7\ntrain_df.loc[train_df['Age'] <=16, 'Age']=  0\ntrain_df.loc[(train_df['Age'] >16) & (train_df['Age'] <=20.125), 'Age']=  1\ntrain_df.loc[(train_df['Age'] >20.125) & (train_df['Age'] <=24.0), 'Age']=  2\ntrain_df.loc[(train_df['Age'] >24) & (train_df['Age'] <=28), 'Age']=  3\ntrain_df.loc[(train_df['Age'] >28) & (train_df['Age'] <=32.312), 'Age']=  4\ntrain_df.loc[(train_df['Age'] >32.312) & (train_df['Age'] <=38), 'Age']=  5\ntrain_df.loc[(train_df['Age'] >38) & (train_df['Age'] <=47), 'Age']=  6\ntrain_df.loc[(train_df['Age'] >47) & (train_df['Age'] <=80), 'Age']=  7\ntrain_df.loc[train_df['Age'] >80, 'Age']\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.loc[test_df['Age'] <=16, 'Age']=  0\ntest_df.loc[(test_df['Age'] >16) & (test_df['Age'] <=20.125), 'Age']=  1\ntest_df.loc[(test_df['Age'] >20.125) & (test_df['Age'] <=24.0), 'Age']=  2\ntest_df.loc[(test_df['Age'] >24) & (test_df['Age'] <=28), 'Age']=  3\ntest_df.loc[(test_df['Age'] >28) & (test_df['Age'] <=32.312), 'Age']=  4\ntest_df.loc[(test_df['Age'] >32.312) & (test_df['Age'] <=38), 'Age']=  5\ntest_df.loc[(test_df['Age'] >38) & (test_df['Age'] <=47), 'Age']=  6\ntest_df.loc[(test_df['Age'] >47) & (test_df['Age'] <=80), 'Age']=  7\ntest_df.loc[test_df['Age'] >80, 'Age']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing set\n#bins = [0, 16, 20.125, 24.0, 28, 32.312, 38, 47, 80]\n#labels = [0, 1, 2, 3, 4, 5, 6, 7]\n#test_df['Age'] = pd.cut(test_df['Age'], bins=bins, labels=labels, right=True, include_lowest=True)\n#test_df['Age'] = test_df['Age'].astype(int) # Converts back to interger ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Disburition of Passenger fare by survival\nsns.displot(train_df, x='Fare', col='Survived', binwidth=80, height=5) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By age groups of 8 bins\ntrain_df['Fare_Cut'] = pd.qcut(train_df['Fare'], 6) #splits a age variable into quantile-based bins(intervals that each contain the apprx same number of data points)\ntest_df['Fare_Cut'] = pd.qcut(test_df['Fare'], 6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['Fare_Cut'], as_index=False)['Survived'].mean().round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reclassifying Fare from 0 to 5\ntrain_df.loc[train_df['Fare'] <=7.775, 'Fare']=  0\ntrain_df.loc[(train_df['Fare'] >7.775) & (train_df['Fare'] <=8.662), 'Fare']=  1\ntrain_df.loc[(train_df['Fare'] >8.662) & (train_df['Fare'] <=14.454), 'Fare']=  2\ntrain_df.loc[(train_df['Fare'] >14.454) & (train_df['Fare'] <=26), 'Fare']=  3\ntrain_df.loc[(train_df['Fare'] >26) & (train_df['Fare'] <=52.369), 'Fare']=  4\ntrain_df.loc[(train_df['Fare'] >52.369) & (train_df['Fare'] <=512.329), 'Fare']=  5\ntrain_df.loc[train_df['Fare'] >512.329, 'Fare']\n\n\ntest_df.loc[test_df['Fare'] <=7.775, 'Fare']=  0\ntest_df.loc[(test_df['Fare'] >7.775) & (test_df['Fare'] <=8.662), 'Fare']=  1\ntest_df.loc[(test_df['Fare'] >8.662) & (test_df['Fare'] <=14.454), 'Fare']=  2\ntest_df.loc[(test_df['Fare'] >14.454) & (test_df['Fare'] <=26), 'Fare']=  3\ntest_df.loc[(test_df['Fare'] >26) & (test_df['Fare'] <=52.369), 'Fare']=  4\ntest_df.loc[(test_df['Fare'] >52.369) & (test_df['Fare'] <=512.329), 'Fare']=  5\ntest_df.loc[test_df['Fare'] >512.329, 'Fare']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Etracting titles from the names\ntrain_df['Name']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Title'] = train_df['Name'].str.split(pat=\",\", expand=True)[1].str.split(pat=\".\", expand=True)[0].apply(lambda x: x.strip()) # lambda fn Removes any leading or trailing whitespace\ntest_df['Title'] = test_df['Name'].str.split(pat=\",\", expand=True)[1].str.split(pat=\".\", expand=True)[0].apply(lambda x: x.strip())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['Title'], as_index=False)['Survived'].mean().round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['Title']).size()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grouping by titles \n#-> Military -- Capt,col, major\n#-> Noble -- Jonhkheer, the countless, Don, Lady, Sir\n#-> Unmarried Female --  mlle, ms, mme","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Title'] = train_df['Title'].replace({\n    'Capt': 'Military',\n    'Col': 'Military',\n    'Major': 'Military',\n    'Jonkheer' : 'Noble',\n    'the Countess': 'Noble',\n    'Don': 'Noble',\n    'Lady': 'Noble',\n    'Sir': 'Noble',\n    'Mlle': 'Noble',\n    'Ms': 'Noble',\n    'Mme': 'Noble'\n    \n})\n\ntest_df['Title'] = test_df['Title'].replace({\n    'Capt': 'Military',\n    'Col': 'Military',\n    'Major': 'Military',\n    'Jonkheer' : 'Noble',\n    'the Countess': 'Noble',\n    'Don': 'Noble',\n    'Lady': 'Noble',\n    'Sir': 'Noble',\n    'Mlle': 'Noble',\n    'Ms': 'Noble',\n    'Mme': 'Noble'\n    \n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Grouping by title\ntrain_df.groupby(['Title'], as_index=False)['Survived'].agg(['count', 'mean']).round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checking the name length (someone important may have a longer name)\ntrain_df['Name_Length'] = train_df['Name'].apply(lambda x: len(x))\ntest_df['Name_Length'] = test_df['Name'].apply(lambda x: len(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#kde plot\ng = sns.kdeplot(train_df['Name_Length'][(train_df['Survived'] == 0) & (train_df['Name_Length'].notnull())], color='Red', fill=True)\ng = sns.kdeplot(train_df['Name_Length'][(train_df['Survived'] == 1) & (train_df['Name_Length'].notnull())], ax=g, color='Blue', fill=True)\ng.set_xlabel('Name_Length')\ng.set_ylabel('Frequency')\ng = g.legend(['Not Survived', 'Survived'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Name_LengthGB'] = pd.qcut(train_df['Name_Length'], 8) #splits a age variable into quantile-based bins(intervals that each contain the apprx same number of data points)\ntest_df['Name_LengthGB'] = pd.qcut(test_df['Name_Length'], 8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['Name_LengthGB'], as_index=False)['Survived'].mean().round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reclassifying age from 0 to 7\ntrain_df.loc[train_df['Name_Length'] <=18, 'Name_Size']=  0\ntrain_df.loc[(train_df['Name_Length'] >18) & (train_df['Name_Length'] <=20), 'Name_Size']=  1\ntrain_df.loc[(train_df['Name_Length'] >20) & (train_df['Name_Length'] <=23), 'Name_Size']=  2\ntrain_df.loc[(train_df['Name_Length'] >23) & (train_df['Name_Length'] <=25), 'Name_Size']=  3\ntrain_df.loc[(train_df['Name_Length'] >25) & (train_df['Name_Length'] <=27.25), 'Name_Size']=  4\ntrain_df.loc[(train_df['Name_Length'] >27.5) & (train_df['Name_Length'] <=30), 'Name_Size']=  5\ntrain_df.loc[(train_df['Name_Length'] >30) & (train_df['Name_Length'] <=38), 'Name_Size']=  6\ntrain_df.loc[(train_df['Name_Length'] >38) & (train_df['Name_Length'] <=82), 'Name_Size']=  7\ntrain_df.loc[train_df['Name_Length'] >82, 'Name_Size']\n\ntest_df.loc[test_df['Name_Length'] <=18, 'Name_Size']=  0\ntest_df.loc[(test_df['Name_Length'] >18) & (test_df['Name_Length'] <=20), 'Name_Size']=  1\ntest_df.loc[(test_df['Name_Length'] >20) & (test_df['Name_Length'] <=23), 'Name_Size']=  2\ntest_df.loc[(test_df['Name_Length'] >23) & (test_df['Name_Length'] <=25), 'Name_Size']=  3\ntest_df.loc[(test_df['Name_Length'] >25) & (test_df['Name_Length'] <=27.25), 'Name_Size']=  4\ntest_df.loc[(test_df['Name_Length'] >27.5) & (test_df['Name_Length'] <=30), 'Name_Size']=  5\ntest_df.loc[(test_df['Name_Length'] >30) & (test_df['Name_Length'] <=38), 'Name_Size']=  6\ntest_df.loc[(test_df['Name_Length'] >38) & (test_df['Name_Length'] <=82), 'Name_Size']=  7\ntest_df.loc[test_df['Name_Length'] >82, 'Name_Size']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Ticket var\ntrain_df['Ticket']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# spliting up the ticket string by whitespace and selects the last part\ntrain_df['TicketNumber'] = train_df['Ticket'].apply(lambda x: pd.Series({'Ticket': x.split()[-1]})) \ntest_df['TicketNumber'] = test_df['Ticket'].apply(lambda x: pd.Series({'Ticket': x.split()[-1]})) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grouping by \ntrain_df.groupby(['TicketNumber'], as_index=False)['Survived'].agg(['count', 'mean']).sort_values('count', ascending = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identfying how many passengers share the same ticket number (fam members could be sharing same ticket numbers)\ntrain_df.groupby('TicketNumber')['TicketNumber'].transform('count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['TicketNumberCounts'] = train_df.groupby('TicketNumber')['TicketNumber'].transform('count')\ntest_df['TicketNumberCounts'] = test_df.groupby('TicketNumber')['TicketNumber'].transform('count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['TicketNumberCounts'], as_index=False)['Survived'].agg(['mean', 'count']).sort_values('count', ascending = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Ticket']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Analyzing the first section of tickets\ntrain_df['Ticket'].str.split(pat=\" \", expand=True) # split on spaces","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#It creates a new array based on whether the second part of the 'Ticket' string (after splitting by space) exists or not.\ntrain_df['TicketLocation'] = np.where(train_df['Ticket'].str.split(pat=\" \", expand=True)[1].notna(), train_df['Ticket'].str.split(pat=\" \", expand=True)[0].apply(lambda x:x.strip()), 'Blank')\ntest_df['TicketLocation'] = np.where(test_df['Ticket'].str.split(pat=\" \", expand=True)[1].notna(), test_df['Ticket'].str.split(pat=\" \", expand=True)[0].apply(lambda x:x.strip()), 'Blank')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def extract_ticket_prefix(ticket):\n#    parts = ticket.split()\n#    if len(parts) > 1:\n#        return parts[0].strip()\n#    else:\n#        return 'Blank'\n\n#train_df['TicketPrefix'] = train_df['Ticket'].apply(extract_ticket_prefix)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['TicketLocation'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classifying the above ticket locations\ntrain_df['TicketLocation'] = train_df['TicketLocation'].replace({\n    'SOTON/O.Q.':'SOTON/OQ',\n    'C.A.':'CA',\n    'CA.':'CA',\n    'SC/PARIS':'SC/Paris',\n    'S.C./PARIS':'SC/Paris',\n    'A/4.':'A/4',\n    'A/5.':'A/5',\n    'A.5.':'A/5',\n    'A./5.':'A/5',\n    'W./C.':'W/C',    \n})\n\ntest_df['TicketLocation'] = test_df['TicketLocation'].replace({\n    'SOTON/O.Q.':'SOTON/OQ',\n    'C.A.':'CA',\n    'CA.':'CA',\n    'SC/PARIS':'SC/Paris',\n    'S.C./PARIS':'SC/Paris',\n    'A/4.':'A/4',\n    'A/5.':'A/5',\n    'A.5.':'A/5',\n    'A./5.':'A/5',\n    'W./C.':'W/C',    \n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['TicketLocation'], as_index=False)['Survived'].agg(['count', 'mean'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cabin analysis\ntrain_df['Cabin'].unique","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Filling missing values in the Cabin column & then extracting the first letter of each cabin entry, which typically represents the deck on the Titanic.\ntrain_df['Cabin'] = train_df['Cabin'].fillna('U') # U for unassigned\ntrain_df['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'x' for i in train_df['Cabin']])\n\ntest_df['Cabin'] = test_df['Cabin'].fillna('U')\ntest_df['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'x' for i in test_df['Cabin']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['Cabin'], as_index=False)['Survived'].agg(['count', 'mean'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#assigns a binary value depending on whether a cabin is known or not.\ntrain_df['Cabin_Assigned'] = train_df['Cabin'].apply(lambda x: 0 if x in  ['U'] else 1)\ntest_df['Cabin_Assigned'] = test_df['Cabin'].apply(lambda x: 0 if x in  ['U'] else 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.groupby(['Cabin_Assigned'], as_index=False)['Survived'].agg(['count', 'mean'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"test_df.info()","metadata":{}},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Imputing missing var (age in both train & test) and fare_cut in test set\ntrain_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\ntest_df['Age'].fillna(test_df['Age'].mean(), inplace=True)\ntest_df['Fare'].fillna(test_df['Fare'].mean(), inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding categorical vars\nohe = OneHotEncoder(sparse_output = False)\node = OrdinalEncoder\nSI = SimpleImputer(strategy = 'most_frequent')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cat vars\node_col = ['Family_Size_Grp']#Ordinal\nohe_col = ['Sex', 'Embarked']#Nominal\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Spilliting outcome (Y) and independent (x)\nX = train_df.drop(['Survived'], axis=1)\ny = train_df['Survived']\nX_test = test_df.drop(['Age_Cut', 'Fare_Cut'], axis=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting train and test\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=835)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating pipelines for imputation & encoding \n# The pipeline function  performs two preprocessing steps, commonly used for preparing categorical ordinal data before feeding it into a machine learning model.\nordinal_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n#This pipeline:\n#Fills in missing values in categorical data.\n#Converts the categories to integers (ordinal encoding), safely handling unknown values during inference.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The pipeline is used to preprocess categorical features, particularly those that don't have an inherent order (i.e., nominal categories).\nohe_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n#This pipeline is great for handling nominal categorical data by:\n#Filling in missing values with the most common category.\n#Converting each category into a set of binary columns (one for each possible category), making the data model-ready.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"col_tran = ColumnTransformer(\n    transformers=[\n        ('impute', SI, ['Age']),                      # Apply the SimpleImputer (SI) to fill missing values in the 'Age' column\n        ('ord_pipeline', ordinal_pipeline, ode_col), # Apply the ordinal_pipeline (imputation + OrdinalEncoder) to ordinal categorical columns\n        ('ohe_pipeline', ohe_pipeline, ohe_col),     # Apply the one-hot encoding pipeline to nominal categorical columns\n        # Pass through these numerical or already-processed columns without any transformation\n        ('passthrough', 'passthrough', [\n            'Pclass',\n            'TicketNumberCounts',\n            'Cabin_Assigned',\n            'Name_Size',\n            'Fare',\n        ])\n    ],\n\n    # Drop any remaining columns not specified above\n    remainder='drop',\n    # Use all available CPU cores to parallelize transformations\n    n_jobs=-1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting a correlation matrix & Heat map\ncorrelation_matrix = train_df.corr(numeric_only=True)\n# Heat map\nplt.figure(figsize=(8,6))# Adjusting the fig size as needed\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Running Models \n## Model 1- Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"# Import and create an instance of the RandomForestClassifier with default parameters\nrfc = RandomForestClassifier()\n\n# Define the hyperparameter grid to search over during model tuning\nparam_grid = {\n    'n_estimators' : [100, 150, 200],    # Number of trees in the forest\n    'min_samples_split': [5,10,15],      # Minimum number of samples required to split an internal node\n    'max_depth':[8,9,10,15,20],          # Maximum depth of the tree (controls overfitting)\n    'min_samples_leaf': [1,2,4],         # Minimum number of samples required to be at a leaf node\n    'criterion': ['gini', 'entropy'],    # The function used to measure the quality of a split: Gini impurity or Information Gain (entropy)\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipefinalrfc = make_pipeline(col_tran, CV_rfc)\npipefinalrfc.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(CV_rfc.best_params_)\nprint(CV_rfc.best_score_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model 2- Decision Tree","metadata":{}},{"cell_type":"code","source":"# Import and create an instance of the DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n\n# Define the grid of hyperparameters to search during model tuning\nparam_grid = {\n    # Minimum number of samples required to split an internal node\n    'min_samples_split': [5, 10, 15],\n    # The maximum depth of the tree to control overfitting\n    'max_depth': [10, 20, 30],\n    # Minimum number of samples required to be at a leaf node\n    'min_samples_leaf': [1, 2, 4],\n    # The function to measure the quality of a split\n    'criterion': ['gini', 'entropy'],\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CV_dtc = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipefinaldtc = make_pipeline(col_tran, CV_dtc)\npipefinaldtc.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(CV_dtc.best_params_)\nprint(CV_dtc.best_score_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***These are the best-performing hyperparameters that were found during the search:***\n\ncriterion: 'entropy' — The tree splits were evaluated using information gain (as opposed to 'gini').\n\nmax_depth: 20 — The maximum depth of the tree was limited to 20 levels, which helps prevent overfitting.\n\nmin_samples_leaf: 4 — A leaf node must have at least 4 samples, again reducing the chance of overfitting.\n\nmin_samples_split: 15 — A node must have at least 15 samples to be split, promoting more robust splits.\n\n***Best Score: 0.81188811188811***\nThis is the best mean cross-validation score (likely accuracy) achieved using the above parameters. \nIt means that across all the validation folds, the model correctly predicted the outcome about 81.19% of the time using those hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"## Model 3- K-Nearest Neighbour (KNN)","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier()  # Define the KNN classifier class (but missing parentheses — should be instantiated)\nparam_grid = {  # Define a dictionary of hyperparameters to be used in a grid search\n    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors to consider for classification\n    'weights': ['uniform', 'distance'],  # Weight function: 'uniform' = all points equal, 'distance' = closer points have more influence\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm to compute nearest neighbors\n    'p': [1, 2],  # Power parameter for the Minkowski metric: 1 = Manhattan, 2 = Euclidean\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CV_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipefinalknn = make_pipeline(col_tran, CV_knn)\npipefinalknn.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(CV_knn.best_params_)\nprint(CV_knn.best_score_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***Best Score: 0.814626***\nThis is the best mean cross-validation score (likely accuracy) achieved using the above parameters. \nIt means that across all the validation folds, the model correctly predicted the outcome about 81.19% of the time using those hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"## Model 4- Support Vector Machine","metadata":{}},{"cell_type":"code","source":"svc = SVC()  # Instantiate a Support Vector Classifier (SVC) from scikit-learn\n\nparam_grid = {  # Define a dictionary of hyperparameters to tune during model selection\n    'C': [100, 10, 1.0, 0.001],  # Regularization parameter: controls trade-off between smooth decision boundary and classifying training points correctly\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']  # Specifies the kernel type to be used in the algorithm\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CV_svc = GridSearchCV(estimator=svc, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipefinalsvc = make_pipeline(col_tran, CV_svc)\npipefinalsvc.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(CV_svc.best_params_)\nprint(CV_svc.best_score_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model 5- Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression()  # Instantiate a Logistic Regression model from scikit-learn\nparam_grid = {  \n    'C': [100, 10, 1.0, 0.001],  # Inverse of regularization strength; smaller values specify stronger regularization\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CV_lr = GridSearchCV(estimator=lr, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipefinallr = make_pipeline(col_tran, CV_lr)\npipefinallr.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(CV_lr.best_params_)\nprint(CV_lr.best_score_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model 6- Naive Bayes","metadata":{}},{"cell_type":"code","source":"gnb = GaussianNB()  # Instantiate a Gaussian Naive Bayes classifier\n\nparam_grid = {\n    'var_smoothing': [0.000000001, 0.00000001],  # Smoothing parameter to account for numerical stability\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CV_gnb = GridSearchCV(estimator=gnb, param_grid=param_grid, cv=StratifiedKFold(n_splits=5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipefinalgnb = make_pipeline(col_tran, CV_gnb)\npipefinalgnb.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(CV_gnb.best_params_)\nprint(CV_gnb.best_score_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making predictions","metadata":{}},{"cell_type":"code","source":"y_pred_rfc=pipefinalrfc.predict(X_test) # for randomforest\ny_pred_dtc=pipefinaldtc.predict(X_test) # decision tree\ny_pred_knn=pipefinalknn.predict(X_test) # Nearest Neighbour\ny_pred_svc=pipefinalsvc.predict(X_test) # Support Vector Machine\ny_pred_lr=pipefinallr.predict(X_test) # Logistic regression model\ny_pred_gnb=pipefinalgnb.predict(X_test) # Naive Bayes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Building submission files for each of the sixx models\nsubmission_rfc = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred_rfc\n})\n\nsubmission_dtc = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred_dtc\n})\n\nsubmission_knn = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred_knn\n})\n\nsubmission_svc = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred_svc\n})\n\nsubmission_lr = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred_lr\n})\n\nsubmission_gnb = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred_gnb\n})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating submission CSV files \nsubmission_rfc.to_csv('/kaggle/working/submission_18_04_rfc.csv', index = False)\nsubmission_dtc.to_csv('/kaggle/working/submission_18_04_dtc.csv', index = False)\nsubmission_knn.to_csv('/kaggle/working/submission_18_04_knn.csv', index = False)\nsubmission_svc.to_csv('/kaggle/working/submission_18_04_svc.csv', index = False)\nsubmission_lr.to_csv('/kaggle/working/submission_18_04_lr.csv', index = False)\nsubmission_gnb.to_csv('/kaggle/working/submission_18_04_gnb.csv', index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}